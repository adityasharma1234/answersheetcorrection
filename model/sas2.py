# -*- coding: utf-8 -*-
"""SAS2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2xEecXZHQ9O90V4PFRnzxbSTgE40iCh
"""

import numpy as np
from scipy.io import loadmat 
import matplotlib.pyplot as plt
from datetime import datetime, date, time
import pandas as pd

from google.colab import files
uploaded = files.upload()

np.random.seed(7)

data = pd.read_table('train.tsv')
data1 = pd.read_table('train_rel_2.tsv')

data = [data, data1]
data = pd.concat(data)


essay_text = data['EssayText']
essay_score = data['Score1']
essay_set = data['EssaySet']

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

sets = [0,1,2,3] # The 4 different classes of Scores

texts = []  # list of text samples
labels_index = {}  # dictionary mapping label name to numeric id
labels = []  # list of label ids

for label in sets:
	label_id = len(labels_index)
	labels_index[label] = label_id
	for t in essay_text[data['Score1']==label]:
		texts.append(t)
		labels.append(label_id)


print('Found %s texts.' % len(texts))

top_words = 5000 #top most-frequent words extracted from the dataset


tokenizer = Tokenizer(nb_words=top_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

max_response_length = 500
data = pad_sequences(sequences, maxlen=max_response_length)

#Convert class vector to binary class matrix, for use with categorical_crossentropy.
#Or in simple words to convert numbers into ONE-HOT Vector for MultiClass classification
labels = to_categorical(np.asarray(labels))

print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)


#Shuffle the dataset 
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

import nltk
nltk.download('stopwords')

nltk.download('punkt')

from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

#Set of stopwords from nltk.corpus package
stop = set(stopwords.words('english'))

word_counts_X = []  # List of word Counts in each example response
for x in essay_text:
    word_counts_X.append(len([i for i in word_tokenize(x) if i not in stop]))

sent_counts = []  # Set of sentence counts in each example response
for x in essay_text:
    sent_counts.append(len(sent_tokenize(x)))

word_counts_X = np.array(word_counts_X)
word_counts_X = word_counts_X[indices]

sent_counts = np.array(sent_counts)
sent_counts = sent_counts[indices]

essay_set = np.array(essay_set)
essay_set = essay_set[indices]

#To check whether the dataset is balanced or not
from collections import Counter
Counter(essay_set)

train_size = int(0.85 * len(data))

X_train = data[:train_size]
X_test = data[train_size:]

set_train = essay_set[:train_size]
set_test = essay_set[train_size:]

sent_count_train = sent_counts[:train_size]
sent_count_test = sent_counts[train_size:]

word_count_test = word_counts_X[train_size:]
word_count_train = word_counts_X[:train_size]

#Concatenate the three data-vectors into a single matrix or feature-set
features_train = np.column_stack((set_train,sent_count_train,word_count_train))
features_test = np.column_stack((set_test,sent_count_test,word_count_test))

y_train = labels[:train_size]
y_test = labels[train_size:]

print(X_train.shape)
print(features_train.shape)

#Model Design
#Functional Model

from keras.layers import Dense, Dropout, Activation, Input
from keras.layers import LSTM as lst
from keras.layers import merge as MER
from keras.models import Model
from keras.layers.embeddings import Embedding
from keras.optimizers import Adam
from keras.layers.advanced_activations import ELU
from keras.layers import Concatenate
from keras.regularizers import l2
from keras.layers import add

#Embedding-Output Vector Length
embedding_vector_length = 32

#Define Text-Input
text_in = Input(shape=(500,), name='text')

#Embeddings
#Used to convert the encoded data i.e the matrix of indices into a form compatible with LSTM
embedding = Embedding(output_dim=embedding_vector_length, input_dim=top_words, input_length=500)(text_in)

#LSTM
lstm_1= lst(100, return_sequences = True)(embedding)
lstm_2 = lst(150)(lstm_1)

#Features Inputs (essay_set, word_counts, sent_counts)
features_in = Input(shape=(3,), name='features')

#Merge layer to merge the output of LSTM and the feature inputs
x = Concatenate(axis=1)([lstm_2, features_in])

#Dropout for the hidden_units to be independent from each other
dropout = Dropout(0.2)(x)

#Hidden Dense Layer
D1 = Dense(150,)(dropout) # try 150 dense after this
ED1 = ELU()(D1)

# Final Dense Output-Layer
score = Dense(4, activation='softmax', name='score')(ED1) 

#model
model = Model([text_in, features_in],[score])

#optimizer
adam = Adam(lr = 0.001)#, decay = 1e-4) 
print(model.summary())

model.compile(optimizer=adam, loss='categorical_crossentropy',metrics=['accuracy'])

#Start Training
model.fit([X_train,features_train], y_train, epochs = 10, batch_size=128, validation_split=0.1)  # we pass one data array per model input

model.save('SAS_model.h5')

#Predict Score For the test set for calculating accuracy
scores = model.evaluate([X_test,features_test], y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

